import streamlit as st

st.set_page_config(page_title="Aprendiendo LangChain | Memoria", page_icon="üí°")

st.header('üí° Memoria')

st.write('''
Los LLMs son, por defecto, son estaticos, lo que significa que no tienen memoria incorporada. Pero a veces necesitamos memoria para implementar aplicaciones como sistemas conversacionales, que pueden tener que recordar informaci√≥n previa proporcionada por el usuario. Afortunadamente, LangChain proporciona varias soluciones de gesti√≥n de memoria, adecuadas para diferentes casos de uso.

Como hemos visto antes, los vector stores se refieren como memoria a largo plazo; en cambio, los m√©todos que veremos en esta secci√≥n se consideran memoria a corto plazo, ya que no persisten una vez que las interacciones est√°n completas.
''')

st.subheader('ConversationBufferMemory')

st.write('''
Esta es la clase de memoria m√°s sencilla y b√°sicamente lo que hace es incluir los mensajes previos en el nuevo mensaje de LLM. Puedes intentar tener una conversaci√≥n con el chatbot, luego hacer preguntas sobre el mensaje anterior y el LLM podr√° responderlas.
''')

st.code('''
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'), temperature=0.0)

memory = ConversationBufferMemory()
        
chain = ConversationChain(llm=llm, memory=memory)

chain.predict(input=prompt)
''')

st.subheader('ConversationBufferWindowMemory')

st.write('''
Por supuesto, la conversaci√≥n puede extenderse y incluir toda la historia del chat en el prompt puede volverse ineficiente en costo, ya que los prompts m√°s largos resultan en un mayor uso de tokens de LLM. Para optimizar este comportamiento, LangChain proporciona otros tres tipos de memoria. La MemoryBufferWindowConversation nos permite decidir cu√°ntos mensajes del historial del chat debe recordar el sistema, utilizando un par√°metro simple:
''')

st.code('''
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'), temperature=0.0)

# 2 es el numero de mensajes almacenados en la memoria
memory = ConversationBufferWindowMemory(k=2)
        
chain = ConversationChain(llm=llm, memory=memory)

chain.predict(input=prompt)
''')

st.subheader('ConversationTokenBufferMemory')

st.write('''
Muy similar a la memoria anterior, el tipo ConversationTokenBufferMemory nos permite ser m√°s espec√≠ficos sobre la cantidad de tokens que queremos usar en nuestros prompts, y contiene el contenido almacenado para cumplir con ese l√≠mite. Debemos pasar el LLM como par√°metro, para calcular el n√∫mero de tokens.
''')

st.code('''
from langchain.chains import ConversationChain
from langchain.memory import ConversationTokenBufferMemory

llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'), temperature=0.0)

# limitamos a 50 tokens
memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)
''')

st.subheader('ConversationSummaryMemory')

st.write('''
Finalmente, si no queremos cortar arbitrariamente la memoria bas√°ndonos en una longitud fija, podemos usar la ConversationSummaryMemory, que a√∫n nos permite definir un l√≠mite de tokens, pero pasa como memoria un resumen de las interacciones anteriores. De esta manera, a√∫n podemos mantener bajo control la memoria a corto plazo mientras retenemos la informaci√≥n m√°s importante.
''')

st.code('''
from langchain.chains import ConversationChain
from langchain.memory import ConversationSummaryMemory

llm = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'), temperature=0.0)

# limitado a 100 tokens and almacenado un resumen
memory = ConversationSummaryMemory(llm=llm, max_token_limit=100)
''')

st.info("Para nuestro proyecto final, utilizaremos la ConversationChain, otra cadena integrada de LangChain. Con el cual podremos elegir el tipo de memoria y comprender el uso de la memoria inspeccionar la memoria usando el metodo `dump`.", icon="‚ÑπÔ∏è")

st.subheader('Referencias: ')

st.markdown('''
- [Conversation Buffer](https://python.langchain.com/docs/modules/memory/types/buffer)
- [Conversation Buffer Window](https://python.langchain.com/docs/modules/memory/types/buffer_window)
- [Conversation Token Buffer](https://python.langchain.com/docs/modules/memory/types/token_buffer)           
- [Conversation Summary](https://python.langchain.com/docs/modules/memory/types/summary)
''')

st.divider()

st.write('Un proyecto hecho por [BlazzByte](https://www.buymeacoffee.com/blazzmocompany) - \
Necesitas IA aprendizaje / Consulta? [Hablanos aqui](mailto:blazzmo.company@gmail.com)')
